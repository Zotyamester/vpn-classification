{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas scikit-learn seaborn matplotlib \"nfstream==6.5.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfstream import NFStreamer, NFPlugin\n",
    "from math import log2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process PCAP files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from Adrián Pekár <apekar@hit.bme.hu>\n",
    "\n",
    "class FlowSlicerWithLabels(NFPlugin):\n",
    "    # Shared state across all flows\n",
    "    tracked_flows = {}\n",
    " \n",
    "    @staticmethod\n",
    "    def normalize_flow_key(src_ip, dst_ip, src_port, dst_port, protocol):\n",
    "        # Normalize 5-tuple to ensure consistent key regardless of direction\n",
    "        if (src_ip, src_port) < (dst_ip, dst_port):\n",
    "            return (src_ip, dst_ip, src_port, dst_port, protocol)\n",
    "        else:\n",
    "            return (dst_ip, src_ip, dst_port, src_port, protocol)\n",
    " \n",
    "    def on_init(self, packet, flow):\n",
    "        # Normalize the 5-tuple key\n",
    "        flow_key = self.normalize_flow_key(\n",
    "            flow.src_ip, flow.dst_ip, flow.src_port, flow.dst_port, flow.protocol\n",
    "        )\n",
    " \n",
    "        # Check if it's the first time this flow's 5-tuple is seen\n",
    "        if flow_key not in FlowSlicerWithLabels.tracked_flows:\n",
    "            FlowSlicerWithLabels.tracked_flows[flow_key] = 1\n",
    "            flow.udps.label = \"first\"\n",
    "        else:\n",
    "            FlowSlicerWithLabels.tracked_flows[flow_key] += 1\n",
    "            flow.udps.label = \"residual\"\n",
    " \n",
    "    def on_update(self, packet, flow):\n",
    "        # Expire the flow if packet limit is reached\n",
    "        if flow.bidirectional_packets >= self.limit:\n",
    "            flow.expiration_id = -1  # Expire the flow to create a new one\n",
    " \n",
    "    def on_expire(self, flow):\n",
    "        # No specific action needed here for labeling\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_entropy(X):\n",
    "    p = {}\n",
    "    # calculate frequencies\n",
    "    for xi in X:\n",
    "        p[xi] = p.get(xi, 0) + 1\n",
    "    # normalize frequencies\n",
    "    for xi in p:\n",
    "        p[xi] /= len(X)\n",
    "\n",
    "    # calculate Sample entropy\n",
    "    return -sum(p[xi] * log2(p[xi] / len(X)) for xi in p)\n",
    "\n",
    "class FingerprintPlugin(NFPlugin):\n",
    "    def __init__(self, win_size, buf_size, u, t, sigma, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.payload_buffers = {}\n",
    "        \n",
    "        self.win_size = win_size\n",
    "        self.buf_size = buf_size\n",
    "        self.u = u\n",
    "        self.t = t\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def processing(self, packet, flow):\n",
    "        # append the hex encoded payload to the flow\n",
    "        # note: packet.ip_packet is a bytes object\n",
    "        if len(self.payload_buffers[flow.id]) < self.buf_size:\n",
    "            self.payload_buffers[flow.id] += packet.ip_packet.hex()\n",
    "        \n",
    "    def on_init(self, packet, flow):\n",
    "        self.payload_buffers[flow.id] = ''\n",
    "        self.processing(packet, flow)\n",
    "        \n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        self.processing(packet, flow)\n",
    "\n",
    "    def on_expire(self, flow):\n",
    "        # if len(self.payload_buffers[flow.id]) < self.buf_size:  # add padding to payload buffer if it does not exceed the buffer size\n",
    "        #     self.payload_buffers[flow.id] += ['\\0'.encode().hex()] * (self.buf_size - len(self.payload_buffers[flow.id]))\n",
    "        actual_buf_size = min(self.buf_size, len(self.payload_buffers[flow.id]))\n",
    "\n",
    "        count = 0\n",
    "        Hf = []\n",
    "        for i in range(actual_buf_size - self.win_size + 1):\n",
    "            win_buffer = self.payload_buffers[flow.id][i : i + self.win_size]\n",
    "            Hi = sample_entropy(win_buffer)\n",
    "            Hf.append(Hi)\n",
    "            if Hi > self.u - self.t * self.sigma:\n",
    "                count += 1\n",
    "\n",
    "        flow.udps.sus = (count == actual_buf_size - self.win_size + 1)\n",
    "        flow.udps.Hf = Hf\n",
    "\n",
    "# parameters\n",
    "IDLE_TIMEOUT = 10000\n",
    "ACTIVE_TIMEOUT = 100000\n",
    "WIN_SIZE = 32  # Luo\n",
    "BUF_SIZE = 1024  # during the analysis\n",
    "U = 4.8817  # Luo\n",
    "T = 3  # to gain 99.4% confidence\n",
    "SIGMA = 0.08134\n",
    "\n",
    "def load_streams_from_pcap(path):\n",
    "    df = NFStreamer(\n",
    "        source=path,\n",
    "        # how to handle packets of a flow\n",
    "        decode_tunnels=True,\n",
    "        idle_timeout=IDLE_TIMEOUT,\n",
    "        active_timeout=ACTIVE_TIMEOUT,\n",
    "        n_dissections=20,\n",
    "        accounting_mode=1,\n",
    "        # what to look for\n",
    "        statistical_analysis=True,\n",
    "        splt_analysis=20,\n",
    "        udps=FingerprintPlugin(WIN_SIZE, BUF_SIZE, U, T, SIGMA),\n",
    "    ).to_pandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct labeling based on filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file_paths = [p for p in Path(\"work/pcaps\").iterdir() if p.is_file() and p.suffix in {\".pcap\", \".pcapng\"}]\n",
    "\n",
    "for p in file_paths:\n",
    "    vpn_type, *_ = p.stem.split(\"_\")\n",
    "    is_vpn = (vpn_type == \"vpn\")\n",
    "    if is_vpn:\n",
    "        traffic = \"vpn\"\n",
    "    else:\n",
    "        traffic = \"web\"\n",
    "\n",
    "    print(f\"Processing: {p.name} (labeled as {traffic} traffic)\")\n",
    "    df = load_streams_from_pcap(p.absolute())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot PLS like in the whitepaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: some hardcore PLS plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 'n stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#  - include everything from NFStreamer except time-related infos, protocols etc.\n",
    "#  - keep PIAT and packet size like stats, SEF, PLS, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = <MODEL_VAR>.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make some nice graphs of the model performance on the testing dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
