{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas scikit-learn seaborn matplotlib \"nfstream==6.5.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfstream import NFStreamer, NFPlugin\n",
    "from math import log2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from pathlib import Path\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process PCAP files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from Adrián Pekár <apekar@hit.bme.hu>\n",
    "\n",
    "def normalize_flow_key(src_ip, dst_ip, src_port, dst_port, protocol):\n",
    "    # Normalize 5-tuple to ensure consistent key regardless of direction\n",
    "    if (src_ip, src_port) < (dst_ip, dst_port):\n",
    "        return (src_ip, dst_ip, src_port, dst_port, protocol)\n",
    "    else:\n",
    "        return (dst_ip, src_ip, dst_port, src_port, protocol)\n",
    "\n",
    "class FlowSlicerWithLabels(NFPlugin):\n",
    "    # Shared state across all flows\n",
    "    tracked_flows = {}\n",
    "    \n",
    "    def __init__(self, limit, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.limit = limit\n",
    " \n",
    "    def on_init(self, packet, flow):\n",
    "        # Normalize the 5-tuple key\n",
    "        flow_key = normalize_flow_key(\n",
    "            flow.src_ip, flow.dst_ip, flow.src_port, flow.dst_port, flow.protocol\n",
    "        )\n",
    " \n",
    "        # Check if it's the first time this flow's 5-tuple is seen\n",
    "        flow.udps.flow_key = flow_key\n",
    "        if flow_key not in FlowSlicerWithLabels.tracked_flows:\n",
    "            FlowSlicerWithLabels.tracked_flows[flow_key] = 1\n",
    "            flow.udps.label = \"first\"\n",
    "        else:\n",
    "            FlowSlicerWithLabels.tracked_flows[flow_key] += 1\n",
    "            flow.udps.label = \"residual\"\n",
    " \n",
    "    def on_update(self, packet, flow):\n",
    "        # Expire the flow if packet limit is reached\n",
    "        if flow.bidirectional_packets >= self.limit:\n",
    "            flow.expiration_id = -1  # Expire the flow to create a new one\n",
    " \n",
    "    def on_expire(self, flow):\n",
    "        # No specific action needed here for labeling\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_entropy(X):\n",
    "    p = {}\n",
    "    # calculate frequencies\n",
    "    for xi in X:\n",
    "        p[xi] = p.get(xi, 0) + 1\n",
    "    # normalize frequencies\n",
    "    for xi in p:\n",
    "        p[xi] /= len(X)\n",
    "\n",
    "    # calculate Sample entropy\n",
    "    return -sum(p[xi] * log2(p[xi] / len(X)) for xi in p)\n",
    "\n",
    "class FingerprintPlugin(NFPlugin):\n",
    "    def __init__(self, win_size, buf_size, u, t, sigma, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.payload_buffers = {}\n",
    "        \n",
    "        self.win_size = win_size\n",
    "        self.buf_size = buf_size\n",
    "        self.u = u\n",
    "        self.t = t\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def processing(self, packet, flow):\n",
    "        # append the hex encoded payload to the flow\n",
    "        # note: packet.ip_packet is a bytes object\n",
    "        if len(self.payload_buffers[flow.id]) < self.buf_size:\n",
    "            self.payload_buffers[flow.id] += packet.ip_packet.hex()\n",
    "        \n",
    "    def on_init(self, packet, flow):\n",
    "        if flow.udps.label == \"residual\":\n",
    "            return\n",
    "        self.payload_buffers[flow.id] = ''\n",
    "        self.processing(packet, flow)\n",
    "        \n",
    "\n",
    "    def on_update(self, packet, flow):\n",
    "        if flow.udps.label == \"residual\":\n",
    "            return\n",
    "        self.processing(packet, flow)\n",
    "\n",
    "    def on_expire(self, flow):\n",
    "        if flow.udps.label == \"residual\":\n",
    "            return\n",
    "        # if len(self.payload_buffers[flow.id]) < self.buf_size:  # add padding to payload buffer if it does not exceed the buffer size\n",
    "        #     self.payload_buffers[flow.id] += ['\\0'.encode().hex()] * (self.buf_size - len(self.payload_buffers[flow.id]))\n",
    "        actual_buf_size = min(self.buf_size, len(self.payload_buffers[flow.id]))\n",
    "\n",
    "        count = 0\n",
    "        Hf = []\n",
    "        for i in range(actual_buf_size - self.win_size + 1):\n",
    "            win_buffer = self.payload_buffers[flow.id][i : i + self.win_size]\n",
    "            Hi = sample_entropy(win_buffer)\n",
    "            Hf.append(Hi)\n",
    "            if Hi > self.u - self.t * self.sigma:\n",
    "                count += 1\n",
    "\n",
    "        flow.udps.sus = (count == actual_buf_size - self.win_size + 1)\n",
    "        flow.udps.Hf = Hf\n",
    "\n",
    "# parameters\n",
    "SLICING_LIMIT = 30\n",
    "IDLE_TIMEOUT = 10000\n",
    "ACTIVE_TIMEOUT = 100000\n",
    "WIN_SIZE = 32  # Luo\n",
    "BUF_SIZE = 1024  # during the analysis\n",
    "U = 4.8817  # Luo\n",
    "T = 3  # to gain 99.4% confidence\n",
    "SIGMA = 0.08134\n",
    "\n",
    "def load_streams_from_pcap(path):\n",
    "    df = NFStreamer(\n",
    "        source=path,\n",
    "        # how to handle packets of a flow\n",
    "        decode_tunnels=True,\n",
    "        idle_timeout=IDLE_TIMEOUT,\n",
    "        active_timeout=ACTIVE_TIMEOUT,\n",
    "        n_dissections=SLICING_LIMIT,\n",
    "        accounting_mode=1,\n",
    "        # what to look for\n",
    "        statistical_analysis=True,\n",
    "        splt_analysis=SLICING_LIMIT,\n",
    "        udps=[FlowSlicerWithLabels(SLICING_LIMIT), FingerprintPlugin(WIN_SIZE, BUF_SIZE, U, T, SIGMA)],\n",
    "    ).to_pandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct labeling based on filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_RUN = False\n",
    "\n",
    "if not CLEAN_RUN and os.path.exists(\"work/flows.csv\"):\n",
    "    df = pd.read_csv(\"work/flows.csv\")\n",
    "else:\n",
    "    file_paths = [p for p in Path(\"work/pcaps\").iterdir() if p.is_file() and p.suffix in {\".pcap\", \".pcapng\"}]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for p in file_paths:\n",
    "        print(f\"Processing: {p.name}\")\n",
    "\n",
    "        vpn_type, l7, *_ = p.stem.split(\"_\")\n",
    "        category = f\"{vpn_type}_{l7}\"\n",
    "\n",
    "        pcap_df = load_streams_from_pcap(p.absolute())\n",
    "        max_flow_key = pcap_df[\"udps.flow_key\"].mode()[0]\n",
    "        pcap_df = pcap_df[pcap_df[\"udps.flow_key\"] == max_flow_key]\n",
    "        pcap_df = pcap_df[pcap_df[\"udps.label\"] == \"first\"]\n",
    "        pcap_df = pcap_df.drop([\"udps.label\", \"udps.flow_key\"], axis=1)\n",
    "        pcap_df[\"category\"] = category\n",
    "        df = pd.concat([df, pcap_df], axis=0)\n",
    "\n",
    "    df.to_csv(\"work/flows.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot PLS like in the whitepaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df[\"splt_ps\"]:\n",
    "    print(len(eval(str(row))), row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df = df[[\"category\", \"splt_ps\"]]\n",
    "\n",
    "packet_positions = list(range(1, SLICING_LIMIT + 1))\n",
    "categories = sorted(list(set(ps_df[\"category\"])))\n",
    "category_colors = ['blue', 'pink', 'green', 'red', 'purple', 'orange', 'black', 'brown', 'cyan', 'magenta', 'yellow', 'gray', 'olive', 'lime', 'teal', 'navy', 'maroon', 'aqua', 'fuchsia', 'silver', 'lime', 'teal', 'navy', 'maroon', 'aqua', 'fuchsia', 'silver']\n",
    "\n",
    "# Generate data for each category\n",
    "data = {}\n",
    "\n",
    "for row in ps_df.itertuples():\n",
    "    if row[1] not in data:\n",
    "        data[row[1]] = ([], [])\n",
    "    data[row[1]][0].append(packet_positions)\n",
    "    data[row[1]][1].append(eval(row[2]))\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "\n",
    "# Plot data points for each category\n",
    "for category, color in zip(categories, category_colors):\n",
    "    positions, lengths = data[category]\n",
    "    plt.scatter(lengths, positions, label=category, color=color)\n",
    "\n",
    "# Add labels, grid, and legend\n",
    "plt.title('PLS')\n",
    "plt.xlabel('Payload Length')\n",
    "plt.ylabel('Packet Position')\n",
    "plt.yticks(packet_positions)\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 'n stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    columns = [\n",
    "        # Packet count (???)\n",
    "        # \"src2dst_packets\", \"dst2src_packets\", \"src2dst_bytes\", \"dst2src_bytes\",\n",
    "\n",
    "        # Packet Size\n",
    "        \"bidirectional_mean_ps\", \"bidirectional_stddev_ps\",\n",
    "        \"src2dst_max_ps\", \"src2dst_min_ps\", \"src2dst_mean_ps\", \"src2dst_stddev_ps\",\n",
    "        \"dst2src_max_ps\", \"dst2src_min_ps\", \"dst2src_mean_ps\", \"dst2src_stddev_ps\",\n",
    "\n",
    "        # Packet Interarrival Time (PIAT)\n",
    "        \"bidirectional_mean_piat_ms\", \"bidirectional_stddev_piat_ms\",\n",
    "        \"bidirectional_max_piat_ms\", \"bidirectional_min_piat_ms\",\n",
    "        \"src2dst_mean_piat_ms\", \"src2dst_stddev_piat_ms\",\n",
    "        \"src2dst_max_piat_ms\", \"src2dst_min_piat_ms\",\n",
    "        \"dst2src_mean_piat_ms\", \"dst2src_stddev_piat_ms\",\n",
    "        \"dst2src_max_piat_ms\", \"dst2src_min_piat_ms\",\n",
    "\n",
    "        # Sample Entropy Fingerprint (SEF)\n",
    "        \"udps.Hf\",\n",
    "        \"udps.sus\",\n",
    "\n",
    "        # Packet Length Sequence (PLS)\n",
    "        \"splt_ps\",\n",
    "\n",
    "        # Label\n",
    "        \"category\",\n",
    "    ]\n",
    "\n",
    "    df = df[columns]\n",
    "    return df\n",
    "\n",
    "df_dropped = extract_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(df):\n",
    "    Hf_avg = df[\"udps.Hf\"].apply(eval).apply(np.mean)\n",
    "    Hf_median = df[\"udps.Hf\"].apply(eval).apply(np.median)\n",
    "    df[\"udps.Hf_avg\"] = Hf_avg\n",
    "    df[\"udps.Hf_med\"] = Hf_median\n",
    "    df = df.drop([\"udps.Hf\"], axis=1)\n",
    "\n",
    "    splt_df = pd.DataFrame(list(eval(x) for x in df[\"splt_ps\"]), columns=[f\"splt_ps_{i}\" for i in range(1, SLICING_LIMIT + 1)])\n",
    "    df = pd.concat([df.drop([\"splt_ps\"], axis=1), splt_df], axis=1)\n",
    "    return df\n",
    "\n",
    "df_flattened = flatten(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df):\n",
    "    le = LabelEncoder()\n",
    "    df[\"category_encoded\"] = le.fit_transform(df[\"category\"])\n",
    "    return df, le\n",
    "\n",
    "df_encoded, label_encoder = encode(df_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop([\"category\", \"category_encoded\"], axis=1)\n",
    "y = df_encoded[\"category_encoded\"]\n",
    "\n",
    "smote = SMOTE(k_neighbors=2, random_state=42)  # Adjust k_neighbors to smallest class size - 1\n",
    "smote_enn = SMOTEENN(random_state=42, smote=smote)\n",
    "X_comb, y_comb = smote_enn.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_comb, y_comb, test_size=0.2, random_state=42, stratify=y_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_frequencies(ds_names, ds, classes):\n",
    "    fig_ds, axes = plt.subplots(1, 2, figsize=(10, 4), dpi=300)  # Two subplots for training and test data\n",
    "    for i, (ds_type, ds_data) in enumerate(zip(ds_names, ds)):\n",
    "        # Assuming ds_data contains the labels (y_train, y_test) as numerical labels\n",
    "        labels = ds_data\n",
    "        bin_edges = np.concatenate(([-.5], np.arange(len(classes)) + .5))  # For correct bin placement in histogram\n",
    "        axes[i].hist(labels, bins=bin_edges, density=False, rwidth=0.6)\n",
    "        axes[i].set_title(f'{ds_type} Labels')\n",
    "        axes[i].set_xticks(np.arange(len(classes)))\n",
    "        axes[i].set_xticklabels(classes)\n",
    "        axes[i].tick_params(axis='x', rotation=90)\n",
    "        axes[i].set_xlabel('Class Labels')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_label_frequencies(\n",
    "    ['Training', 'Test'],   # Names of the datasets\n",
    "    [y_train, y_test],      # Label data for training and testing\n",
    "    label_encoder.classes_  # Class names or labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = rf_clf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "cm = confusion_matrix(y_test, test_predictions, normalize='true')\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=300)\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Blues')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 3), dpi=300)\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "X_train.columns[indices.tolist()[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_flow(name, path):\n",
    "    category = name\n",
    "\n",
    "    pcap_df = load_streams_from_pcap(path)\n",
    "    flows = {}\n",
    "    for _, flow in pcap_df.iterrows():\n",
    "        flow_key = flow[\"udps.flow_key\"]\n",
    "        if flow_key not in flows:\n",
    "            flows[flow_key] = 1\n",
    "        else:\n",
    "            flows[flow_key] += 1\n",
    "    max_flow = max(flows, key=flows.get)\n",
    "    pcap_df = pcap_df.loc[lambda x: x[\"udps.flow_key\"] == max_flow]\n",
    "    pcap_df[\"category\"] = category\n",
    "    pcap_df = pcap_df[pcap_df[\"udps.label\"] == \"first\"]\n",
    "    pcap_df = pcap_df.drop([\"udps.label\", \"udps.flow_key\"], axis=1)\n",
    "    return pcap_df\n",
    "\n",
    "custom_df, _ = encode(flatten(extract_features(load_custom_flow(\"nonvpn_sftp\", \"work/pcaps/sftptest.pcapng\"))))\n",
    "# custom_df, _ = encode(flatten(extract_features(dani(\"vpn_youtube\", \"work/pcaps/urbanvpn_youtube.pcapng\"))))\n",
    "custom_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_custom_test = custom_df.drop([\"category\", \"category_encoded\"], axis=1)\n",
    "y_custom_test = custom_df[\"category_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_test_predictions = rf_clf.predict(X_custom_test)\n",
    "custom_test_predictions = label_encoder.inverse_transform(custom_test_predictions)\n",
    "print(f\"Predicted: {custom_test_predictions.max()}, actual: {custom_df['category'].max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
